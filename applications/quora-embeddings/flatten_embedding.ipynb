{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"quora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 808580 training examples\n",
      "Found 537933 unique training examples\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def flatten_data(dataset):\n",
    "    for pairs in dataset[\"train\"][\"questions\"]:\n",
    "        for id, text in zip(pairs[\"id\"], pairs[\"text\"]):\n",
    "            yield {\"id\": id, \"text\": text}\n",
    "\n",
    "\n",
    "# The only goal here is to flatten the data into a single set. \n",
    "# Notice that there's only half as many unique training examples.\n",
    "df = pd.DataFrame(flatten_data(dataset))\n",
    "print(f\"Found {len(df)} training examples\")\n",
    "df.drop_duplicates(subset=\"id\", inplace=True)\n",
    "print(f\"Found {len(df)} unique training examples\")\n",
    "df.to_csv(\"quora_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text\n",
       "0   1  What is the step by step guide to invest in sh...\n",
       "1   2  What is the step by step guide to invest in sh...\n",
       "2   3  What is the story of Kohinoor (Koh-i-Noor) Dia...\n",
       "3   4  What would happen if the Indian government sto...\n",
       "4   5  How can I increase the speed of my internet co..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This should be the set of texts we should just pass and tune embedding:\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import asyncio\n",
    "from typing import List, Literal, Tuple\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "def batched(iterable, n=1):\n",
    "    \"\"\"\n",
    "    Yields batches of size n from iterable\n",
    "    \"\"\"\n",
    "    it = iter(iterable)\n",
    "    while True:\n",
    "        chunk = list(itertools.islice(it, n))\n",
    "        if not chunk:\n",
    "            return\n",
    "        yield chunk\n",
    "\n",
    "\n",
    "class Embedder:\n",
    "    @classmethod\n",
    "    def batch_text(cls, texts: List[Tuple[int, str]], batch_size: int = 32):\n",
    "        for batch in batched(texts, batch_size):\n",
    "            yield batch\n",
    "\n",
    "    @classmethod\n",
    "    async def embed_openai(\n",
    "        cls,\n",
    "        chunks: List[Tuple[int, str]],\n",
    "        model: Literal[\n",
    "            \"text-embedding-3-small\", \"text-embedding-3-large\"\n",
    "        ] = \"text-embedding-3-small\",\n",
    "    ):\n",
    "        client = AsyncOpenAI()\n",
    "        sem = asyncio.Semaphore(32)\n",
    "\n",
    "        # There's an opportunity to cache this single method\n",
    "        # Also an opportunity to add retry logic.\n",
    "        async def fetch_embedding(idz: int, text_str: str):\n",
    "            async with sem:\n",
    "                response = await client.embeddings.create(input=text_str, model=model)\n",
    "                return (idz, response.data[0].embedding)\n",
    "\n",
    "        results = await asyncio.gather(\n",
    "            *[fetch_embedding(idz, text_str) for (idz, text_str) in chunks]\n",
    "        )\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.0020987512543797493, 0.0270865336060524, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.001020575873553753, 0.013290978036820889, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.009630665183067322, 0.010176597163081169, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.029207905754446983, -0.0005261672777123749...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[-0.02165737748146057, -0.010240409523248672, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            embedding\n",
       "id                                                   \n",
       "1   [-0.0020987512543797493, 0.0270865336060524, 0...\n",
       "2   [0.001020575873553753, 0.013290978036820889, 0...\n",
       "3   [-0.009630665183067322, 0.010176597163081169, ...\n",
       "4   [-0.029207905754446983, -0.0005261672777123749...\n",
       "5   [-0.02165737748146057, -0.010240409523248672, ..."
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_id_str():\n",
    "    \"\"\"\n",
    "    This is just how we can convert the train data set into any generic data set we want.\n",
    "    Now this is something that can be supported regardless of the type. we can imagine\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(\"quora_train.csv\")\n",
    "    sample = df.head(50)  # Obviously we should do more in practice.\n",
    "    sample_tuples = zip(sample[\"id\"], sample[\"text\"])\n",
    "    return sample_tuples\n",
    "\n",
    "\n",
    "async def embed(sample_tuples):\n",
    "    \"\"\"\n",
    "    This can be a generic function that takes any kinds of arrays and embeds them.\n",
    "    Right now we'll use OpenAI, but we should also use Hugging Face inference server.\n",
    "\n",
    "    Now that embeddings we have new openai models, we should ignore cohere and just do inference server\n",
    "    and openai.\n",
    "    \"\"\"\n",
    "    results = await Embedder.embed_openai(\n",
    "        chunks=sample_tuples, model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    df = pd.DataFrame(results, columns=[\"id\", \"embedding\"]).set_index(\"id\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Notice that now we just have a table that is ID and embedding. This is the format we want.\n",
    "# This is effectively what will be in a vector database. So it will be familiar to everyone.\n",
    "sample_tuples = generate_id_str()\n",
    "embeddings_df = await embed(sample_tuples)\n",
    "embeddings_df.to_csv(\"quora_train_embeddings.csv\", index=True)\n",
    "embeddings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['questions', 'is_duplicate'],\n",
       "        num_rows: 404290\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4s/sh67w10s36jcb0f4b9gvmrdr0000gn/T/ipykernel_96989/1126469944.py:11: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  e1, e2 = embeddings.iloc[id1][0], embeddings.iloc[id2][0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1</th>\n",
       "      <th>id2</th>\n",
       "      <th>e1</th>\n",
       "      <th>e2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.001020575873553753, 0.013290978036820889, 0...</td>\n",
       "      <td>[-0.009630665183067322, 0.010176597163081169, ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>[-0.029207905754446983, -0.0005261672777123749...</td>\n",
       "      <td>[-0.02165737748146057, -0.010240409523248672, ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>[0.03239667788147926, 0.02922770380973816, 0.0...</td>\n",
       "      <td>[0.0016469762194901705, -0.05773146450519562, ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.049847107380628586, -0.010537532158195972, ...</td>\n",
       "      <td>[0.0366658940911293, 0.02135019563138485, -0.0...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.043602462857961655, 0.020671047270298004, 0...</td>\n",
       "      <td>[-0.005765771958976984, -0.018585262820124626,...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id1  id2                                                 e1  \\\n",
       "0    1    2  [0.001020575873553753, 0.013290978036820889, 0...   \n",
       "1    3    4  [-0.029207905754446983, -0.0005261672777123749...   \n",
       "2    5    6  [0.03239667788147926, 0.02922770380973816, 0.0...   \n",
       "3    7    8  [0.049847107380628586, -0.010537532158195972, ...   \n",
       "4    9   10  [0.043602462857961655, 0.020671047270298004, 0...   \n",
       "\n",
       "                                                  e2  is_duplicate  \n",
       "0  [-0.009630665183067322, 0.010176597163081169, ...         False  \n",
       "1  [-0.02165737748146057, -0.010240409523248672, ...         False  \n",
       "2  [0.0016469762194901705, -0.05773146450519562, ...         False  \n",
       "3  [0.0366658940911293, 0.02135019563138485, -0.0...         False  \n",
       "4  [-0.005765771958976984, -0.018585262820124626,...         False  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def new_dataset(embeddings: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    This is really a helper, but the goal will just be to take the embedding\n",
    "    table and the dataset and produce a new table that has all these pairs.\n",
    "    \"\"\"\n",
    "    dataset = load_dataset(\"quora\")\n",
    "    question = dataset[\"train\"]\n",
    "    for data in question:\n",
    "        try:\n",
    "            id1, id2 = data[\"questions\"][\"id\"]\n",
    "            e1, e2 = embeddings.iloc[id1][0], embeddings.iloc[id2][0]\n",
    "            yield (id1, id2, e1, e2, data[\"is_duplicate\"])\n",
    "        except Exception as e:\n",
    "            break\n",
    "\n",
    "\n",
    "# Notice how simple it is to produce this pairwise data set.\n",
    "# Now you can do all your complex stuff to get the training and test splits done correctly. \n",
    "df = pd.DataFrame(\n",
    "    new_dataset(embeddings_df), columns=[\"id1\", \"id2\", \"e1\", \"e2\", \"is_duplicate\"]\n",
    ")\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
